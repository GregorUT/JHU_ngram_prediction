---
title: "Miltestone"
author: "Gregory Smith"
date: "March 20, 2016"
output: html_document
---

#Milestone - Corpus Cleanup & Analysis

## Corpus creation and cleanup
```{r setup, include=FALSE}
#knitr options.
knitr::opts_chunk$set(cache=TRUE)
```

Required Libraries
```{r, echo=FALSE}
# Prerequisites and file config
library(plyr)
library(tm)
library(slam)
library(RWeka)
library(reshape2)
library(stringr)
library(klaR)
library(caret)
library(ggplot2)

WorkingDir <- ("D:/Repository/Capstone/")
DataDir <- ("D:/Repository/Capstone/Dataset/")
# Data has been downloaded and extracted to the Datasets folder
DataLang <- "en_US."
FileNews <- paste(c(DataDir, DataLang, "news.txt"), collapse = "")
FileBlog <- paste(c(DataDir, DataLang, "blogs.txt"), collapse = "")
FileTwitter <- paste(c(DataDir, DataLang, "twitter.txt"), collapse = "")
ProfanityList <- readLines("D:/Repository/Capstone/Dataset/Profanity/Swearwords.csv")
```

###Corpus creation 
After some initial cleanup of the twitter data(due do some possible file corruption or special characters not common to English), the three files are read into a volatile Corpora using the VCorpus call, part of the tm library.

The three files are comprised of:

en_US.blogs.txt    Size -  205,235 kb      Entries  -   899,288

en_US.news.txt     Size -  200,989 kb      Entries  -    77,259   

en_US.twitter.txt  Size -  163,735 kb      Entries  - 2,360,151 


##Raw Corpus Info
The size of the corpus in memory is 599.4 Meg.
It took 35.45 seconds to read in the corpus.

###Steps to clean data
*Remove Numbers
*Transfer to lowercase
*Remove Punctuation
*Remove profanity
*Limit all remaining characters to alpha
*Remove white space

Since "stopwords" are a vital part of the English language the will be necessary for the model.

The Twitter file contains several unusual characters.  Although these could be cleaned with the rest of the corpus, for timing efficiency they will be cleaned prior to reading into the corpus.

## Determine the sample size
The amount of data contained in the raw corpus is substantially more data than can be efficiently used.

###With an inital sample set at 5%
Unique Words - 108,097    

Bi-grams     - 1,111,191      2+ occurrences - 257,306   10+ occurrences   36,085

Tri-grams    - 2,308,081      2+ occurrences - 235,449   10+ occurrences   16,205

4-grams      - 2,765,625      2+ occurrences -  94,605   10+ occurrences    2,233

5-gram       - 2,768,124      2+ occurrences -  27,587   10+ occurrences      242

###With the sample set at 7.5%
Unique Words - 137,880    

Bi-grams     - 1,523,245      2+ occurrences - 365,178   10+ occurrences   53,667

Tri-grams    - 3,332,343      2+ occurrences - 365,170   10+ occurrences   27,661

4-grams      - 4,107,512      2+ occurrences - 160,973   10+ occurrences    4,429

5-grams      - 4,155,420      2+ occurrences -  50,016   10+ occurrences      566

###With the sample set at 10%

unique words - 163,605     

Bi-grams     - 1,896,807      2+ occurrences - 463,956   10+ occurrences   70,548

Tri-grams    - 4,303,158      2+ occurrences - 493,656   10+ occurrences   39,291

4-grams      - 5,408,322      2+ occurrences - 229,837   10+ occurrences    7,032

5-grams      - 5,513,363      2+ occurrences -  70,548   10+ occurrences    1,005

###With the sample set at 12.5%

Unique words - 186,173

Bi-grams     -2,241,504       2+ occurrences - 558,512   10+ occurrences   86,474

Tri-grams    -5,231,169       2+ occurrences - 621,953   10+ occurrences    51,361

4-grams          *                              *                          *

5-grams          *                              *                          *

with 12.5% size sample NGramTokenizer returned out of memory error while tokenizing 4-gram and 5-gram.

###with the sample set at 15%

Unique words 208,337       

Bi-grams     *                  *                       *

Tri-grams    *                  *                       *

4-grams      *                  *                       *

5-grams      *                  *                       *

With 15% size sample NGramTokenizer returned out of memory error.

##Sample Size Summary 
Since additional information is added as the corpus size increases. The model will be based on a 10% sample size corpus.
Since the desktop creating the Document-Term-Matrix has 4 Gigs allocated to the heap size, the 10% corpus should allow a sizable
DTM to use for the model.

If DTM proves to be too big for efficient use, the occurrence of the n-gram can be used as a variable.

##Word Counts With 10% sample Rates

Blog    - 2874194

News    - 210692

Twitter - 2278898 

##The Twitter Impact
Through the exploration of data, it becomes apparent that certain phrases and terms occur at a much higher probability than would occur in natural language usage.  In order to offset the "Twitter Bias".  The weight of the various models will need to be adjusted to more closer match natural language usage.

Examples - 
The most common 4-gram is

Thanks for the follow

The most common 3-gram is 

Thanks for the

While exploring the break down of a 3 gram 

"good night" bi-gram leads to "twitter" 23.4% of occurrences.

The nature of auto response in twitter is influencing the model.

##Adjusting the sample rates for the corpus
To balance the smaller news corpus, the news sample will be weighted with a multiple of 4

To counter the "Twitter Bias", the twitter sample will be weighted with a multiple of .5

The Blog sample weight will remain the same.

The sample percentage is 10%.

The weight for the blog is 1.

The weight for the news is 4.

the weight for the twitter is 0.5.

#Creation of the Term Document Matrix
             Total Number of Words
Blog                2874194

News                 840846

Twitter             1135336

```{r, echo = FALSE}
load(file = "FullData.rda")
```



##Determine Percentage of words covered by cut count
```{r, echo=FALSE}
CoverageCompute<- function(x)
{
        TotalCount <- sum(x$count)
        TotalEntries <- nrow(x)
        coverageList <- NULL
        StepCount <- TotalEntries/20
        stepnum <- 1
        rIndex <- round(StepCount, digits = 0)
        while (rIndex <= TotalEntries)
        {
                coverage <- sum(x$count[1:rIndex])/TotalCount
                #Change from string to matrix to allow for graphs.
                newEntry <- c(stepnum * 5, coverage)
                coverageList <- rbind(coverageList,newEntry)
                stepnum <- stepnum +1
                rIndex <- rIndex + StepCount
                                  
        }
        rownames(coverageList) <- NULL
        colnames(coverageList) <- c("Frequent","Coverage")
        coverageList <- as.data.frame(coverageList)
}

Coverage.single <- CoverageCompute(gramsStats.single)
Coverage.bi <- CoverageCompute(gramsStats.bi)
Coverage.tri <- CoverageCompute(gramsStats.tri)
Coverage.four <- CoverageCompute(gramsStats.four)
Coverage.five <- CoverageCompute(gramsStats.five)
```

#Corpus Coverage by Frequent Words
The top 10 frequent words. 
```{r}
head(gramsStats.single, 10)
```

Coverage is the percentage of total words in the corpus covered.
The sample of the corpus contains about 150,000 unique words
The top 10% of most frequently used words cover 92.8% of the words used in the corpus.
```{R, echo=FALSE}
qplot( Frequent, Coverage, data=Coverage.single, main = "Corpus Coverage by Frequent words as a percentage", xlab = "Frequent Words as %", ylab = "Coverage of total Words as %", geom ="line")
```


#Bi-gram Coverage by percentage
The top 10 bi-grams are:
```{r}
head(gramsStats.bi, 10)
```

Frequent Bi-grams is the percentage of top occurring bi-grams.
Coverage is the percentage of total bi-grams in the corpus covered by the percentage of 
```{R, echo=FALSE}
qplot( Frequent, Coverage, data=Coverage.bi, main = "Bi-gram Coverage by Frequent Bigrams as a percentage", xlab = "Frequent Bi-Grams as %", ylab = "Coverage of total Bi-Grams as %", geom = "line")
```

```{r, echo=FALSE}
determindiff <-function(x,x_count)
{
        delta_x <- x$Frequent[x_count:20]/100
        delta_y <- x$Coverage[x_count:20]
        df <- diff(delta_x)/diff(delta_y)
        return (df)
} 

slope.bi <- mean(determindiff(Coverage.bi, 5))
```
After including the top 25% of frequent bi-grams with a coverage of about 78% of the corpus.  The rate of coverage tapers off to 3.34 % coverage for every 1 percent of frequent bi-grams added.

Suggested cut-point at 25% of frequent top bi-grams.

#Tri-gram Coverage by percentage
The top 10 tri-grams are:
```{r}
head(gramsStats.tri, 10)
```

Frequent tri-grams is the percentage of top occurring bi-grams.
Coverage is the percentage of total tri-grams in the corpus covered by the percentage of 
```{R, echo=FALSE}
qplot( Frequent, Coverage, data=Coverage.tri, main = "tri-gram Coverage by Frequent Bigrams as a percentage", xlab = "Frequent tri-Grams as %", ylab = "Coverage of total tri-Grams as %")
```

After including the top 15% of frequent bi-grams with a coverage of about 40% of the corpus.  The rate of coverage tapers off to `r round(mean(determindiff(Coverage.tri, 3)),3)` coverage for every 1 percent of frequent bi-grams added.

##Modeling the data
The primary concern with the determincation of how many occurances of a given n-gram should be kept and which should be cut.  The smaller the cut number the higher the acuracy of the model but also the larger the dataset.  Finding the correct balance will be key to the success of the model.

#Prediction algorithm
The prediction model will be based on the a backoff process of looking at the first three words of a 4-gram.  If there is a match compare the frequencies of the matches.  If there isn't a match it will fall back to the tri-gram, etc.

